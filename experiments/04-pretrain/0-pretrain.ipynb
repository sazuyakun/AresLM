{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88445653",
   "metadata": {},
   "source": [
    "# 1. Using GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b140b5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from gpt.gpt_model import GPTModel\n",
    "from config.config import GPT_CONFIG_124M\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a1c5eb",
   "metadata": {},
   "source": [
    "We reduce the context length (context_length) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens\n",
    "\n",
    "* Next, we use the generate_text_simple function from the previous chapter to generate text\n",
    "\n",
    "* In addition, we define two convenience functions, text_to_token_ids and token_ids_to_text, for converting between token and text representations that we use throughout this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65307c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from utils.utils import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dead666",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Calculating the text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f397f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbc417",
   "metadata": {},
   "source": [
    "Feeding the inputs to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fcfb954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "# Applying the softmax function, we can turn the logits tensor into a tensor of the same dimension containing probability scores\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d10a3",
   "metadata": {},
   "source": [
    "We can apply argmax function to convert the probability scores into predicted token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c260573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac45dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888b9f7",
   "metadata": {},
   "source": [
    "To train the model, we need to know how far it is away from the correct predictions (targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d35b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6dc7c4",
   "metadata": {},
   "source": [
    "* We want to maximize all these values, bringing them close to a probability of 1\n",
    "* In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa37a18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ec683",
   "metadata": {},
   "source": [
    "Next, we compute the average log probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a6ed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3ecb0",
   "metadata": {},
   "source": [
    "* The goal is to make this average log probability as large as possible by optimizing the model weights\n",
    "* Due to the log, the largest possible value is 0, and we are currently far away from 0\n",
    "---\n",
    "\n",
    "* In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0\n",
    "* The value negative of -10.7722, i.e., 10.7722, is also called cross-entropy loss in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9e8371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f87dae",
   "metadata": {},
   "source": [
    "PyTorch already implements a cross_entropy function that carries out the previous steps\n",
    "\n",
    "* Before we apply the cross_entropy function, let's check the shape of the logits and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b8262a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits Shape:  torch.Size([2, 3, 50257])\n",
      "Targets Shape:  torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits Shape: \", logits.shape)\n",
    "print(\"Targets Shape: \", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6a40e",
   "metadata": {},
   "source": [
    "For the cross_entropy function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc09203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits:  torch.Size([6, 50257])\n",
      "Flattened targets:  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits: \", logits_flat.shape)\n",
    "print(\"Flattened targets: \", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd005ce",
   "metadata": {},
   "source": [
    "* Note that the targets are the token IDs, which also represent the index positions in the logits tensors that we want to maximize\n",
    "* The `cross_entropy` function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f97b051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d4f40",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "* A concept related to the cross-entropy loss is the perplexity of an LLM\n",
    "* The perplexity is simply the exponential of the cross-entropy loss\n",
    "\n",
    "> The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 52,918 words or tokens)\n",
    "\n",
    "#### Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60c5d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3bdb85",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Calculating the training and validation set losses\n",
    "\n",
    "We use a relatively small dataset for training the LLM (in fact, only one short story)\n",
    "\n",
    "> FACT: Llama 2 7B required 184,320 GPU hours on A100 GPUs to be trained on 2 trillion tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be86a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# we are going to use the same dataset from data loading\n",
    "file_path = \"../../data/the-verdict.txt\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98e7d612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60cf6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2003527",
   "metadata": {},
   "source": [
    "##### Next, we divide the dataset into a training and a validation set and use the data loaders from chapter 2 to prepare the batches for LLM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35071cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloader import create_dataloader_v1\n",
    "\n",
    "# train-test splitting\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    text=train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    text=val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bd4410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a73aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f0240e",
   "metadata": {},
   "source": [
    "### We will implement a utility function to calculate the cross-entropy loss of a given batch\n",
    "* In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b2aeb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7482912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n",
      "Training loss: 10.98758316040039\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "model = model.to(device)\n",
    "# model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3631b",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20e181fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "\n",
    "    # Initialize list to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set the model to training mode\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}:\")\n",
    "        for input_batch, target_batch in tqdm(train_loader):\n",
    "            # Reset the loss gradients from prev. batch iterations\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Note: that the input_batch and the target_batch are already in the designated device\n",
    "            #       as seen in the code for calc_loss_batch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()     # Compute loss gradients\n",
    "\n",
    "            optimizer.step()    # Update Model weights using loss gradients\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional Evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    device,\n",
    "                    eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "995f1959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:16<02:09, 16.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.817, Val loss 9.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [05:31<02:46, 55.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [07:15<00:00, 48.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Epoch: 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [01:28<05:09, 44.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2 (Step 000010): Train loss 6.619, Val loss 7.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [06:31<02:05, 62.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2 (Step 000015): Train loss 6.046, Val loss 6.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [08:59<00:00, 59.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,, and,, the,, the, and,, and,,, the, and,,,,,,\n",
      "Epoch: 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [02:52<06:13, 62.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3 (Step 000020): Train loss 5.524, Val loss 6.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [07:26<00:52, 52.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3 (Step 000025): Train loss 5.369, Val loss 6.378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [08:13<00:00, 54.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you, and to the of the of the picture. Gis.                                     \n",
      "Epoch: 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [04:04<05:12, 62.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 (Step 000030): Train loss 4.830, Val loss 6.263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [09:37<00:00, 64.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 (Step 000035): Train loss 4.586, Val loss 6.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you of the \"I the picture.                    \"I\"I the picture\"I had the picture\"I the picture and I had been the picture of\n",
      "Epoch: 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [03:22<02:49, 42.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5 (Step 000040): Train loss 3.879, Val loss 6.130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [05:52<00:00, 39.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know he had been his pictures, and I felt it's by his last word.                   \"Oh, and he had been the end, and he had been\n",
      "Epoch: 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:36<04:52, 36.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6 (Step 000045): Train loss 3.530, Val loss 6.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [04:19<02:18, 46.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6 (Step 000050): Train loss 2.960, Val loss 6.123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [06:27<00:00, 43.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know it was his pictures--I glanced after him, I had the last word.        \"Oh, and I was his pictures--I looked.   \"I looked. \"I looked. \n",
      "Epoch: 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [01:38<05:55, 50.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7 (Step 000055): Train loss 2.832, Val loss 6.150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [09:30<03:02, 91.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7 (Step 000060): Train loss 2.104, Val loss 6.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [12:11<00:00, 81.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know the picture to me--I glanced after him, and Mrs.  \"I was no great, the fact, the fact that, the moment--as Jack himself, as his pictures--as of the picture--because he was a little\n",
      "Epoch: 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [04:44<09:44, 97.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8 (Step 000065): Train loss 1.691, Val loss 6.186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [11:52<01:29, 89.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8 (Step 000070): Train loss 1.391, Val loss 6.230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [13:27<00:00, 89.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little: \"Yes--and by me to me to have to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Epoch: 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [05:54<07:09, 85.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9 (Step 000075): Train loss 1.059, Val loss 6.251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [16:21<00:00, 109.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9 (Step 000080): Train loss 0.800, Val loss 6.278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"  He laughed again, and threw back the window-curtains, I saw that, and down the room, and now\n",
      "Epoch: 10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [13:27<10:53, 163.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10 (Step 000085): Train loss 0.569, Val loss 6.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [18:40<00:00, 124.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a66d9dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 110.15 minutes.\n"
     ]
    }
   ],
   "source": [
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41631d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gpt_final_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the final trained model to a file\n",
    "model_save_path = \"gpt_final_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c42496da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from gpt_final_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model from file\n",
    "loaded_model = GPTModel(GPT_CONFIG_124M)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "print(\"Model loaded from\", model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "903a8f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word, on Mrs. Thwing's the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--because he didn't want to go on painting;\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sample(\n",
    "    loaded_model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    \"The word, on Mrs. Thwing's\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b5f62b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.png\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2fe7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
